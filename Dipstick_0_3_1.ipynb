{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dipstick 0.3.1",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HWYissTXp6LVxG4qfKfK3xJoEVKVO1sh",
      "authorship_tag": "ABX9TyMJRx466YEJi3zUIT6kztvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conceptbin/dipstick/blob/master/Dipstick_0_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7nlh4KlZWZL"
      },
      "source": [
        "#User input\n",
        "Enter user variables in this section, then run all cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibzEvEhHZnh2"
      },
      "source": [
        "search_term = \"Roehampton\"\n",
        "limit = 1000 #If you're just testing the search, set a smaller limit. The higher the limit, the longer the search takes to run."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFiFYTLMZWCR"
      },
      "source": [
        "#Search and save to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdLrLf_jWZfO"
      },
      "source": [
        "#Install Twint for Twitter search\n",
        "!pip3 install twint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un-oeOpmB3l7"
      },
      "source": [
        "import pandas as pd\n",
        "import twint\n",
        "\n",
        "c = twint.Config()\n",
        "c.Search = search_term\n",
        "c.Limit = limit\n",
        "#c.Min_likes = 5 #Minimum number of likes, to just get tweets people interacted with.\n",
        "c.Pandas = True\n",
        "\n",
        "twint.run.Search(c)\n",
        "df = twint.storage.panda.Tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INm7MZL2YH5-"
      },
      "source": [
        "#Basic Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1prG8lbdRxK-"
      },
      "source": [
        "##User by frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6vrSALdRwVX"
      },
      "source": [
        "most_tweets = df.groupby(['username']).size().reset_index(name='counts')\n",
        "most_tweets = most_tweets.sort_values(by='counts', ascending=False)\n",
        "most_tweets[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNGS7lXDPjTl"
      },
      "source": [
        "##Likes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfImp7OMOlna"
      },
      "source": [
        "#Most likes\n",
        "most_l = (df.nlargest(1000, 'nlikes') \n",
        "          .drop_duplicates(['tweet'])\n",
        "          )\n",
        "most_l = most_l.sort_values(by='nlikes', ascending=False)\n",
        "most_l[:10][['date','username','tweet','nlikes']]  #Slice of list, selected columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXWdh1mYPf3-"
      },
      "source": [
        "##Retweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtkLLz_zNOgf"
      },
      "source": [
        "#Most retweeted\n",
        "most_r = (df.nlargest(1000, 'nretweets') \n",
        "          .drop_duplicates(['tweet'])\n",
        "          )\n",
        "most_r[:10][['date','username','tweet','nretweets']]  #Slice of list, selected columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLDEBnlwNUXC"
      },
      "source": [
        "##Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM_Jal5wu7pj"
      },
      "source": [
        "#Overview data\n",
        "tweets_total = len(df)  #Total no. of tweets in the set\n",
        "tweeters = len(df['username'].unique())  #No. of unique tweeters\n",
        "time_from = df['date'].min()\n",
        "time_to = df['date'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdFvCIlKlOu"
      },
      "source": [
        "#Gather overview data into a dict\n",
        "report = {'What': ['Total no. of tweets:', 'No. of unique tweeters:', 'Time from:', 'Time to:'],\n",
        "          'Number': [tweets_total, tweeters, time_from, time_to]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_EgSAXLkgg"
      },
      "source": [
        "#Make overview dataframe from dict and display\n",
        "report_table = pd.DataFrame(report)\n",
        "report_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ImZ7AV_s2e"
      },
      "source": [
        "#Keywords, hashtags, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktkiI7j-BFZl"
      },
      "source": [
        "##N-grams\n",
        "Code adapted from De Dios, From Dataframe to N-Grams (Medium 22 May 2020) [link text](https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-wdXcDjBfwT"
      },
      "source": [
        "#Import libraries \n",
        "\n",
        "# natural language processing: n-gram ranking\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmDK5Z57BgGv"
      },
      "source": [
        "#Function for stripping stopwords, lemmatizing, removing punctuation.\n",
        "def basic_clean(text):\n",
        "  \"\"\"\n",
        "  Cleans the text data by removing stopwords, lemmatizing after encoding,\n",
        "  punctuation removed with regex parsing. Returns a list of words.\n",
        "  \"\"\"\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "    .encode('ascii', 'ignore')\n",
        "    .decode('utf-8', 'ignore')\n",
        "    .lower())\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  return [wnl.lemmatize(word) for word in words if word not in stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzYIaP8VB1Px"
      },
      "source": [
        "#try out basic_clean\n",
        "words = basic_clean(''.join(str(df['tweet'].tolist())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxJb2pSWB041"
      },
      "source": [
        "#Top 10 bigrams\n",
        "(pd.Series(nltk.ngrams(words, 2)).value_counts())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt89seykDKf8"
      },
      "source": [
        "#top 10 trigrams\n",
        "(pd.Series(nltk.ngrams(words, 3)).value_counts())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2bczcDuDy4d"
      },
      "source": [
        "##Visualization of N-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqY6ULmoDrzU"
      },
      "source": [
        "bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]\n",
        "trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KumPxougFU4_"
      },
      "source": [
        "bigrams_series.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73L_fY2tEcDe"
      },
      "source": [
        "###Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDv-jsWcD2nA"
      },
      "source": [
        "bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "plt.title('20 Most Frequently Occuring Bigrams')\n",
        "plt.ylabel('Bigram')\n",
        "plt.xlabel('# of Occurences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHx6OalKE7aO"
      },
      "source": [
        "###Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkDBc_EAEKY4"
      },
      "source": [
        "trigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "plt.title('20 Most Frequently Occuring Trigrams')\n",
        "plt.ylabel('Trigram')\n",
        "plt.xlabel('# of Occurences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iApBWCaAaTjO"
      },
      "source": [
        "##Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJq61SDaQ30"
      },
      "source": [
        "def tag_clean(text):\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  #stopwords = nltk.corpus.stopwords.words('english')\n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "    .encode('ascii', 'ignore')\n",
        "    .decode('utf-8', 'ignore')\n",
        "    .lower())\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  return [wnl.lemmatize(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_dz_UvuyYDL"
      },
      "source": [
        "#try out tag_clean\n",
        "tags = tag_clean(''.join(str(df['hashtags'].tolist())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgipwTnayqbn"
      },
      "source": [
        "#top hashtags\n",
        "hashtags_series = (pd.Series(tags).value_counts())[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbmxh-7C1SQS"
      },
      "source": [
        "##Visualization of top hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdJWYufu1QqF"
      },
      "source": [
        "hashtags_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "plt.title('30 Most Frequent Hashtags')\n",
        "plt.ylabel('Hashtag')\n",
        "plt.xlabel('# of Occurences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGE1t_kFpSLw"
      },
      "source": [
        "# Output\n",
        "Saves everything to an Excel sheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6olDd6VPZmq"
      },
      "source": [
        "#Save dataframes to separate sheets in an Excel workbook.\n",
        "with pd.ExcelWriter('dipstick_out.xlsx') as writer:\n",
        "  report_table.to_excel(writer, sheet_name='Overview report')\n",
        "  most_l.to_excel(writer, sheet_name='Most likes')\n",
        "  most_r.to_excel(writer, sheet_name='Most retweets')\n",
        "  most_tweets.to_excel(writer, sheet_name='Most tweets by user')\n",
        "  bigrams_series.to_excel(writer, sheet_name='Top bigrams (stopwords removed')\n",
        "  trigrams_series.to_excel(writer, sheet_name='Top trigrams')\n",
        "  hashtags_series.to_excel(writer, sheet_name='Top hashtags')\n",
        "  df.to_excel(writer, sheet_name='All tweets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtDaa_NE6kS5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}